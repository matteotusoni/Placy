{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3901cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import json\n",
    "from itertools import product\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d2bb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = ['stat', 'nonstat', 'stat_stocklike', 'nonstat_stocklike']\n",
    "causal_strengths = [.2, .5, 1.]\n",
    "n_vars = [5, 10]\n",
    "s_gs = [.5, 1.]\n",
    "s_bs = [.0, .1, .5, 1.]  #  2., 5.\n",
    "\n",
    "methods = {\n",
    "    'Granger': ['time', 'freq'], \n",
    "    'PCMCI': ['time'],\n",
    "    'CCM-Filtering': ['time'],\n",
    "    'RCV-VarLiNGAM': ['time'],\n",
    "    'Rhino': ['time'],\n",
    "    'Dynotears': ['time'],\n",
    "    'PCMCIomega': ['time'],\n",
    "    # 'Geweke': ['time'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7318748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def m2label(m_tf):\n",
    "    return {\n",
    "        'Granger_time': '\\\\textbf{Granger}',\n",
    "        'Granger_freq': '\\\\textbf{PLACy}',\n",
    "        'PCMCI_time': '\\\\textbf{PCMCI}',\n",
    "        'CCM-Filtering_time': '\\\\textbf{CCM-Filtering}',\n",
    "        'RCV-VarLiNGAM_time': '\\\\textbf{RCV-VarLiNGAM}',\n",
    "        'Rhino_time': '\\\\textbf{Rhino}',\n",
    "        'Dynotears_time': '\\\\textbf{DYNOTEARS}',\n",
    "        'PCMCIomega_time': '\\\\textbf{PCMCI}$_{\\\\boldsymbol{\\\\Omega}}$',\n",
    "        'Geweke_time': '\\\\textbf{Geweke}',\n",
    "    }[m_tf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85435c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_index(means, stds):\n",
    "    max_mean = max(means)\n",
    "    max_indices = [i for i, m in enumerate(means) if m == max_mean]\n",
    "    if len(max_indices) == 0:\n",
    "        return []\n",
    "    if len(max_indices) == 1:\n",
    "        return [max_indices[0]]\n",
    "    else:\n",
    "        min_std = min(stds[i] for i in max_indices)\n",
    "        result = [i for i in max_indices if stds[i] == min_std]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4780a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_table_short(n_var, s_g):\n",
    "    todo_dyno = list()\n",
    "    todo_rhino = list()\n",
    "    todo_other = list()\n",
    "\n",
    "    zero_division = np.nan\n",
    "    methods_tf = [f'{method}_{tf}' for method in methods for tf in methods[method]]\n",
    "    columns = [m2label(m_tf) for m_tf in methods_tf]\n",
    "    index = pd.MultiIndex.from_tuples(product(data_types, causal_strengths), names=['', '\\\\textbf{C}'])\n",
    "    df = pd.DataFrame(index=index, columns=columns)\n",
    "\n",
    "    for causal_strength in causal_strengths:\n",
    "        for data_type in data_types:\n",
    "            for c in methods_tf:\n",
    "                method, tf = c.split('_')\n",
    "                f1 = list()\n",
    "                for s_b in s_bs:\n",
    "                    for seed in range(10 if method == 'Rhino' else 100):\n",
    "                        fname = f'results/{method}_{data_type}/n_vars={n_var}/causal_strength={causal_strength}/s_g={s_g}/s_b={s_b}/seed={seed}.json'\n",
    "                        try:\n",
    "                            with open(fname, 'r') as f:\n",
    "                                data = json.load(f)\n",
    "                            cm_est_tf = np.asarray(data[f'cm_est_{tf}']).ravel()\n",
    "                            cm_true = np.asarray(data['cm']).ravel()\n",
    "                            if causal_strength != 0:\n",
    "                                f1.append(f1_score(cm_true, cm_est_tf, zero_division=zero_division))\n",
    "                        except FileNotFoundError:\n",
    "                            stationary = 'nonstat' not in data_type\n",
    "                            stock_like = 'stocklike' in data_type\n",
    "                            x = f'{n_var} {causal_strength} {s_g} {s_b} {seed} {stationary} {stock_like} {method}'\n",
    "                            if method == 'Rhino':\n",
    "                                todo_rhino.append(x)\n",
    "                            elif method == 'Dynotears':\n",
    "                                todo_dyno.append(x)\n",
    "                            else:\n",
    "                                if method != 'CCM-Filtering' or n_var != 10:\n",
    "                                    todo_other.append(x)\n",
    "                if len(f1) == 0:\n",
    "                    x = np.nan\n",
    "                else:\n",
    "                    mean, std = np.nanmean(f1), np.nanstd(f1)\n",
    "                    x = f'{mean:.2f}{{\\\\tiny$\\pm${std:.2f}}}'\n",
    "                    # x = f'{mean:.2f} $\\pm$ {std:.2f}'\n",
    "                df.loc[(data_type, causal_strength), m2label(c)] = x\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        means = list(map(lambda x: float(x.split('{')[0]) if isinstance(x, str) else x, row.values))\n",
    "        std = list(map(lambda x: float(x.split('m$')[1][:-1]) if isinstance(x, str) else x, row.values))\n",
    "        max_indices = find_max_index(means, std)\n",
    "        for max_index in max_indices:\n",
    "            best = row.values[max_index]\n",
    "            c = columns[row.values.tolist().index(best)]\n",
    "            best = '\\\\textbf{' + best[:4] + '}' + best[4:]\n",
    "            df.loc[index, c] = best\n",
    "\n",
    "    s = df.to_latex(na_rep='---', float_format=\"%.2f\", caption=f'F1 Score - $N={n_var}, \\ \\sigma_g={s_g}$', label=f'tab:N{n_var}sigmag{s_g}')\n",
    "    s = s[:13] + '\\n\\\\renewcommand{\\\\arraystretch}{2}\\centering\\n\\scriptsize' + s[13:]\n",
    "    s = s.replace('00000', '').replace('llllllllll', 'cccccccccc')\n",
    "    s = s.replace('\\multirow[t]{3}{*}{stat}', '\\multirow{3}{*}{\\\\rotatebox{90}{${\\\\rm{OU}}(\\\\sigma_g^m = 0)$}}')\n",
    "    s = s.replace('\\multirow[t]{3}{*}{stat_stocklike}', '\\multirow{3}{*}{\\\\rotatebox{90}{${\\\\rm{OU}}(\\\\sigma_g^m > 0)$}}')\n",
    "    s = s.replace('\\multirow[t]{3}{*}{nonstat}', '\\multirow{3}{*}{\\\\rotatebox{90}{$\\\\widehat{{\\\\rm{OU}}}(\\\\sigma_g^m = 0)$}}')\n",
    "    s = s.replace('\\multirow[t]{3}{*}{nonstat_stocklike}', '\\multirow{3}{*}{\\\\rotatebox{90}{$\\\\widehat{{\\\\rm{OU}}}(\\\\sigma_g^m > 0)$}}')\n",
    "    s = s.replace('\\multirow[t]', '\\\\multirow[c]')\n",
    "    s = s.replace('\\\\cline{1-10}', '\\\\cmidrule(lr){1-10}')\n",
    "    s = s.replace('\\\\cmidrule(lr){1-10}\\n\\\\bottomrule', '\\\\bottomrule')\n",
    "    os.makedirs('tables', exist_ok=True)\n",
    "    with open(f'tables/results_n_var={n_var}_s_g={s_g}_short.tex', 'w') as f:\n",
    "        f.write(s)\n",
    "    return todo_rhino, todo_dyno, todo_other, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0befcfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = make_table_short(5, 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd53e095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_table(n_var, s_g):\n",
    "    todo_dyno = list()\n",
    "    todo_rhino = list()\n",
    "    todo_other = list()\n",
    "\n",
    "    zero_division = np.nan\n",
    "    methods_tf = [f'{method}_{tf}' for method in methods for tf in methods[method]]\n",
    "    columns = [m2label(m_tf) for m_tf in methods_tf]\n",
    "    index = pd.MultiIndex.from_tuples(product(data_types, causal_strengths, s_bs), names=['Dataset', 'C', '$\\sigma_b$'])\n",
    "    df = pd.DataFrame(index=index, columns=columns)\n",
    "\n",
    "    for causal_strength in causal_strengths:\n",
    "        for s_b in s_bs:\n",
    "            for data_type in data_types:\n",
    "                for c in methods_tf:\n",
    "                    method, tf = c.split('_')\n",
    "                    f1 = list()\n",
    "                    for seed in range(10 if method == 'Rhino' else 100):\n",
    "                        fname = f'results/{method}_{data_type}/n_vars={n_var}/causal_strength={causal_strength}/s_g={s_g}/s_b={s_b}/seed={seed}.json'\n",
    "                        try:\n",
    "                            with open(fname, 'r') as f:\n",
    "                                data = json.load(f)\n",
    "                            cm_est_tf = np.asarray(data[f'cm_est_{tf}']).ravel()\n",
    "                            cm_true = np.asarray(data['cm']).ravel()\n",
    "                            if causal_strength != 0:\n",
    "                                f1.append(f1_score(cm_true, cm_est_tf, zero_division=zero_division))\n",
    "                        except FileNotFoundError:\n",
    "                            stationary = 'nonstat' not in data_type\n",
    "                            stock_like = 'stocklike' in data_type\n",
    "                            x = f'{n_var} {causal_strength} {s_g} {s_b} {seed} {stationary} {stock_like} {method}'\n",
    "                            if method == 'Rhino':\n",
    "                                todo_rhino.append(x)\n",
    "                            elif method == 'Dynotears':\n",
    "                                todo_dyno.append(x)\n",
    "                            else:\n",
    "                                if method != 'CCM-Filtering' or n_var != 10:\n",
    "                                    todo_other.append(x)\n",
    "                    if len(f1) == 0:\n",
    "                        x = np.nan\n",
    "                    else:\n",
    "                        mean, std = np.nanmean(f1), np.nanstd(f1)\n",
    "                        x = f'{mean:.2f}{{\\\\tiny$\\pm${std:.2f}}}'\n",
    "                        # x = f'{mean:.2f} $\\pm$ {std:.2f}'\n",
    "                    df.loc[(data_type, causal_strength, s_b), m2label(c)] = x\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        means = list(map(lambda x: float(x.split('{')[0]) if isinstance(x, str) else x, row.values))\n",
    "        std = list(map(lambda x: float(x.split('m$')[1][:-1]) if isinstance(x, str) else x, row.values))\n",
    "        max_indices = find_max_index(means, std)\n",
    "        for max_index in max_indices:\n",
    "            best = row.values[max_index]\n",
    "            c = columns[row.values.tolist().index(best)]\n",
    "            best = '\\\\textbf{' + best[:4] + '}' + best[4:]\n",
    "            df.loc[index, c] = best\n",
    "\n",
    "    s = df.to_latex(na_rep='---', float_format=\"%.2f\", caption=f'F1 Score - $N={n_var}, \\ \\sigma_g={s_g}$', label=f'tab:N{n_var}sigmag{s_g}')\n",
    "    s = s[:13] + '\\n\\centering\\n\\scriptsize' + s[13:]\n",
    "    s = s.replace('00000', '').replace('llllllllll', 'cccccccccc')\n",
    "    s = s.replace('\\cline{1-11} \\cline{2-11}', '\\midrule')\n",
    "    s = s.replace('\\multirow[t]{12}{*}{stat}', '\\multirow{12}{*}{\\\\rotatebox{90}{${\\\\rm{OU}}(\\\\sigma_g^m = 0)$}}')\n",
    "    s = s.replace('\\multirow[t]{12}{*}{stat_stocklike}', '\\multirow{12}{*}{\\\\rotatebox{90}{${\\\\rm{OU}}(\\\\sigma_g^m > 0)$}}')\n",
    "    s = s.replace('\\multirow[t]{12}{*}{nonstat}', '\\multirow{12}{*}{\\\\rotatebox{90}{$\\\\widehat{{\\\\rm{OU}}}(\\\\sigma_g^m = 0)$}}')\n",
    "    s = s.replace('\\multirow[t]{12}{*}{nonstat_stocklike}', '\\multirow{12}{*}{\\\\rotatebox{90}{$\\\\widehat{{\\\\rm{OU}}}(\\\\sigma_g^m > 0)$}}')\n",
    "    s = s.replace('\\multirow[t]', '\\\\multirow[c]')\n",
    "    s = s.replace('\\\\midrule\\n\\\\bottomrule', '\\\\bottomrule')\n",
    "    s = s.replace('\\\\cline{2-11}', '\\\\cmidrule(lr){2-11}')\n",
    "    os.makedirs('tables', exist_ok=True)\n",
    "    with open(f'tables/results_n_var={n_var}_s_g={s_g}.tex', 'w') as f:\n",
    "        f.write(s)\n",
    "    return todo_rhino, todo_dyno, todo_other, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40bcb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "todo_rhino1, todo_dyno1, todo_other1, df1 = make_table(5, .5)\n",
    "todo_rhino2, todo_dyno2, todo_other2, df2 = make_table(5, 1.)\n",
    "todo_rhino3, todo_dyno3, todo_other3, df3 = make_table(10, 0.5)\n",
    "todo_rhino4, todo_dyno4, todo_other4, df4 = make_table(10, 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36290b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "todo_rhino = todo_rhino1 + todo_rhino2 + todo_rhino3 + todo_rhino4\n",
    "todo_dyno = todo_dyno1 + todo_dyno2 + todo_dyno3 + todo_dyno4\n",
    "todo_other = todo_other1 + todo_other2 + todo_other3 + todo_other4\n",
    "len(todo_rhino), len(todo_dyno), len(todo_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea23c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_run = 'other'\n",
    "todo = todo_other\n",
    "len(todo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf6bc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1000\n",
    "for i in range(0, len(todo), chunk_size):\n",
    "    chunk = todo[i:i + chunk_size]\n",
    "    with open(f'params_{i // chunk_size + 1}.txt', 'w') as f:\n",
    "        for item in chunk:\n",
    "            f.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7c9437",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = 'dyno' if to_run == 'dyno' else 'CD'\n",
    "sif_name = 'causalnex_latest.sif' if to_run == 'dyno' else 'python_cd.sif'\n",
    "for i in range(len(todo) // chunk_size + 1):\n",
    "    with open(f'job_{i + 1}.sh', 'w') as f:\n",
    "        f.write(\n",
    "f'''#!/bin/bash\n",
    "#SBATCH --job-name={job_name}\n",
    "#SBATCH --output=logs/output_%A_%a.out\n",
    "#SBATCH --error=logs/error_%A_%a.err\n",
    "#SBATCH --partition=department_only,students\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --gpus=0\n",
    "#SBATCH --mincpus=1\n",
    "\n",
    "params=($(sed -n \"${{SLURM_ARRAY_TASK_ID}}p\" params_{i+1}.txt))\n",
    "\n",
    "n_vars=${{params[0]}}\n",
    "causal_strength=${{params[1]}}\n",
    "s_g=${{params[2]}}\n",
    "s_b=${{params[3]}}\n",
    "seed=${{params[4]}}\n",
    "stationary=${{params[5]}}\n",
    "stock_like=${{params[6]}}\n",
    "method=${{params[7]}}\n",
    "\n",
    "singularity exec {sif_name} python src/run.py --n_vars $n_vars --causal_strength $causal_strength --s_g $s_g --s_b $s_b --seed $seed --stationary $stationary --stock_like $stock_like --method $method\n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45ab75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rhino\n",
    "l = list()\n",
    "for x in todo_rhino2:\n",
    "    n_var, causal_strength, s_g, s_b, seed, stationary, stock_like, method = x.split()\n",
    "    stationary = stationary == 'True'\n",
    "    stock_like = stock_like == 'True'\n",
    "    data_type = 'stat' if stationary else 'nonstat'\n",
    "    if stock_like:\n",
    "        data_type += '_stocklike'\n",
    "    l.append(f'n_vars={n_var}_causal_strength={causal_strength}_s_g={s_g}_s_b={s_b}_{data_type}_seed={seed}')\n",
    "with open(f'data_rhino.txt', 'w') as f:\n",
    "    for item in l:\n",
    "        f.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b6bec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def tnr_score(y_true, y_pred):\n",
    "    tn, fp, _, _ = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "f1 = list()\n",
    "tnr = list()\n",
    "for seed in range(17):\n",
    "    with open(f'results/PCMCIomega_AirQuality/seed={seed}.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "    cm = np.asarray(data['cm']).ravel()\n",
    "    cm_est_time = np.asarray(data['cm_est_time']).ravel()\n",
    "    f1.append(f1_score(cm, cm_est_time, zero_division=np.nan))\n",
    "    tnr.append(tnr_score(cm, cm_est_time))\n",
    "print('PCMCIomega on AirQuality')\n",
    "print('F1:\\t', np.mean(f1).round(2), ' pm ', np.std(f1).round(2))\n",
    "print('TNR:\\t', np.mean(tnr).round(2), ' pm ', np.std(tnr).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293be530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def tnr_score(y_true, y_pred):\n",
    "    tn, fp, _, _ = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "n_var = 5\n",
    "s_g = 1.\n",
    "todo = list()\n",
    "for causal_strength in causal_strengths:\n",
    "    print(f'C: {causal_strength}')\n",
    "    for data_type in data_types:\n",
    "        method, tf = 'Geweke', 'time'\n",
    "        f1 = list()\n",
    "        tnr = list()\n",
    "        for s_b in s_bs:\n",
    "            for seed in range(100):\n",
    "                fname = f'results/{method}_{data_type}/n_vars={n_var}/causal_strength={causal_strength}/s_g={s_g}/s_b={s_b}/seed={seed}.json'\n",
    "                try:\n",
    "                    with open(fname, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "                    cm_est_tf = np.asarray(data[f'cm_est_{tf}']).ravel()\n",
    "                    cm_true = np.asarray(data['cm']).ravel()\n",
    "                    if causal_strength != 0:\n",
    "                        f1.append(f1_score(cm_true, cm_est_tf, zero_division=np.nan))\n",
    "                        tnr.append(tnr_score(cm_true, cm_est_tf))\n",
    "                except FileNotFoundError:\n",
    "                    stationary = 'nonstat' not in data_type\n",
    "                    stock_like = 'stocklike' in data_type\n",
    "                    x = f'{n_var} {causal_strength} {s_g} {s_b} {seed} {stationary} {stock_like} {method}'\n",
    "                    todo.append(x)\n",
    "        if len(f1) == 0:\n",
    "            x = np.nan\n",
    "            y = np.nan\n",
    "        else:\n",
    "            f1_mean, f1_std = np.nanmean(f1), np.nanstd(f1)\n",
    "            tnr_mean, tnr_std = np.nanmean(tnr), np.nanstd(tnr)\n",
    "            x = f'{f1_mean:.2f} $\\pm$ {f1_std:.2f}'\n",
    "            y = f'{tnr_mean:.2f} $\\pm$ {tnr_std:.2f}'\n",
    "        print(f'data_type: {data_type} F1:{x}  TNR:{y}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fd0b51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CausalDiscovery",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
